{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsunei/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import tensorflow_backend\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import scipy.linalg as LA\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras import models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options = tf.GPUOptions(allow_growth = True))\n",
    "session = tf.Session(config = config)\n",
    "tensorflow_backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
    "    return [os.path.join(root, f)\n",
    "            for root, _, files in os.walk(directory) for f in files\n",
    "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工物画像のテストデータを取得してます...\n",
      "人工物画像のテストデータを取得しました\n",
      "自然物画像のテストデータを取得してます...\n",
      "自然物画像のテストデータを取得しました\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_vgg16_trained_model.h5'\n",
    " \n",
    "### データセットの準備\n",
    "                                                    \n",
    "image_size = 224\n",
    " \n",
    "#X = []\n",
    "#Y = []\n",
    "X_eva = []\n",
    "Y_eva = []\n",
    "    \n",
    "#print(\"人工物画像の教師データを取得してます...\")\n",
    "#for filepath in list_pictures('../data/CNN_data/test/j/'):\n",
    " #   warnings.filterwarnings('ignore')\n",
    "  #  img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))        \n",
    "   # X.append(img_eva)\n",
    "    #Y.append(0) # 教師データ（正解）\n",
    "\n",
    "#print(\"人工物画像の教師データを取得しました\")\n",
    "\n",
    "    \n",
    "# 学習データの取得（非正解画像）\n",
    "#print(\"自然物画像の教師データを取得してます...\")\n",
    "#for filepath in list_pictures('../data/CNN_data/test/s/'):\n",
    " #   img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))\n",
    "  #  X.append(img_eva)\n",
    "   # Y.append(1) # 教師データ（正解）\n",
    "\n",
    "#print(\"自然物画像の教師データを取得しました\")\n",
    "    \n",
    "# 学習データの取得（正解画像）\n",
    "#print(\"人工物画像のテストデータを取得してます...\")\n",
    "#for filepath in list_pictures('../data/CNN_data/test/j_test/'):\n",
    " #   img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))        \n",
    "  #  X_eva.append(img_eva)\n",
    "   # Y_eva.append(0) # 教師データ（正解）\n",
    "\n",
    "#print(\"人工物画像のテストデータを取得しました\")\n",
    "\n",
    "    \n",
    "# 学習データの取得（非正解画像）\n",
    "#print(\"自然物画像のテストデータを取得してます...\")\n",
    "#for filepath in list_pictures('../data/CNN_data/test/s_test/'):\n",
    " #   img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))\n",
    "  #  X_eva.append(img_eva)\n",
    "   # Y_eva.append(1) # 教師データ（正解）\n",
    "\n",
    "#print(\"自然物画像のテストデータを取得しました\")\n",
    "\n",
    "print(\"人工物画像のテストデータを取得してます...\")\n",
    "for filepath in list_pictures('../data/CNN_data/test/jinkou/'):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))        \n",
    "    X_eva.append(img_eva)\n",
    "    Y_eva.append(0) # 教師データ（正解）\n",
    "\n",
    "print(\"人工物画像のテストデータを取得しました\")\n",
    "\n",
    "    \n",
    "# 学習データの取得（非正解画像）\n",
    "print(\"自然物画像のテストデータを取得してます...\")\n",
    "for filepath in list_pictures('../data/CNN_data/test/sizen/'):\n",
    "    img_eva = img_to_array(load_img(filepath, target_size=(224, 224)))\n",
    "    X_eva.append(img_eva)\n",
    "    Y_eva.append(1) # 教師データ（正解）\n",
    "\n",
    "print(\"自然物画像のテストデータを取得しました\")\n",
    "\n",
    "#X = np.array(X)   \n",
    "#Y = np.array(Y)          \n",
    "X_eva = np.array(X_eva)   \n",
    "Y_eva = np.array(Y_eva)  \n",
    " \n",
    "#img_gray = 0.299 * X[:, :, :, 0] + 0.587 * X[:, :, :, 1] + 0.114 * X[:, :, :, 2]\n",
    "img_gray2 = 0.299 * X_eva[:, :, :, 0] + 0.587 * X_eva[:, :, :, 1] + 0.114 * X_eva[:, :, :, 2]\n",
    "\n",
    "#X[:, :, :, 0] = img_gray[:]\n",
    "#X[:, :, :, 1] = img_gray[:]\n",
    "#X[:, :, :, 2] = img_gray[:]\n",
    "X_eva[:, :, :, 0] = img_gray2[:]\n",
    "X_eva[:, :, :, 1] = img_gray2[:]\n",
    "X_eva[:, :, :, 2] = img_gray2[:]\n",
    "    \n",
    "#x_train = X.astype('float32')\n",
    "x_test = X_eva.astype('float32')\n",
    "#x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# ｙ　ラベルをワンホット表現に\n",
    "#y_train = keras.utils.to_categorical(Y, num_classes)\n",
    "#y_test = keras.utils.to_categorical(Y_eva, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5296 - acc: 0.7662 - val_loss: 0.3255 - val_acc: 0.9372\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2461 - acc: 0.9353 - val_loss: 0.1758 - val_acc: 0.9447\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1615 - acc: 0.9416 - val_loss: 0.1393 - val_acc: 0.9572\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1407 - acc: 0.9537 - val_loss: 0.1261 - val_acc: 0.9613\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1243 - acc: 0.9578 - val_loss: 0.1142 - val_acc: 0.9641\n",
      "人工物画像の正解枚数： 389 / 409\n",
      "自然物画像の正解枚数： 380 / 391\n",
      "人工物画像の正解率＝ 95.11002444987776 %\n",
      "自然物画像の正解率＝ 97.18670076726342 %\n",
      "全体画像の正解率＝ 96.14836260857058 %\n",
      "(800, 224, 224, 3)\n",
      "Train on 1600 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5646 - acc: 0.7244 - val_loss: 0.3485 - val_acc: 0.9387\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2652 - acc: 0.9272 - val_loss: 0.1830 - val_acc: 0.9563\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1683 - acc: 0.9500 - val_loss: 0.1399 - val_acc: 0.9556\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1368 - acc: 0.9575 - val_loss: 0.1221 - val_acc: 0.9613\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1244 - acc: 0.9606 - val_loss: 0.1122 - val_acc: 0.9637\n",
      "人工物画像の正解枚数： 388 / 413\n",
      "自然物画像の正解枚数： 375 / 387\n",
      "人工物画像の正解率＝ 93.94673123486683 %\n",
      "自然物画像の正解率＝ 96.89922480620154 %\n",
      "全体画像の正解率＝ 95.42297802053419 %\n",
      "(800, 224, 224, 3)\n",
      "Train on 1600 samples, validate on 1600 samples\n",
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5132 - acc: 0.7791 - val_loss: 0.3139 - val_acc: 0.9431\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.2347 - acc: 0.9450 - val_loss: 0.1692 - val_acc: 0.9528\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1483 - acc: 0.9566 - val_loss: 0.1254 - val_acc: 0.9628\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1282 - acc: 0.9609 - val_loss: 0.1104 - val_acc: 0.9669\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.1154 - acc: 0.9613 - val_loss: 0.1016 - val_acc: 0.9684\n",
      "人工物画像の正解枚数： 382 / 401\n",
      "自然物画像の正解枚数： 381 / 399\n",
      "人工物画像の正解率＝ 95.26184538653366 %\n",
      "自然物画像の正解率＝ 95.48872180451127 %\n",
      "全体画像の正解率＝ 95.37528359552246 %\n",
      "(800, 224, 224, 3)\n",
      "トータル正解率：95.65% (+/- 0.35%)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "ave = 0\n",
    "fold_num = 3\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(x_test, Y_eva):\n",
    "    ### モデル構築 \n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "    #vgg_conv.summary()\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(vgg_conv.layers[0])\n",
    "    model.add(vgg_conv.layers[1])\n",
    "    model.add(vgg_conv.layers[2])\n",
    "    model.add(vgg_conv.layers[3])\n",
    "    model.add(vgg_conv.layers[4])\n",
    "    model.add(vgg_conv.layers[5])\n",
    "    model.add(vgg_conv.layers[6])\n",
    "    model.add(vgg_conv.layers[7])\n",
    "    model.add(vgg_conv.layers[8])\n",
    "    model.add(vgg_conv.layers[9])\n",
    "    model.add(vgg_conv.layers[10])\n",
    "    model.add(vgg_conv.layers[11])\n",
    "    model.add(vgg_conv.layers[12])\n",
    "    model.add(vgg_conv.layers[13])\n",
    "    model.add(vgg_conv.layers[14])\n",
    "    model.add(vgg_conv.layers[15])\n",
    "    model.add(vgg_conv.layers[16])\n",
    "    model.add(vgg_conv.layers[17])\n",
    "    model.add(vgg_conv.layers[18])\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))   ### データは４種類\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    ### 学習\n",
    "    model.fit(x_test[train], np_utils.to_categorical(Y_eva[train], 2), batch_size=batch_size, epochs=epochs, validation_data=(x_test[train], np_utils.to_categorical(Y_eva[train], 2)), shuffle=True)\n",
    "\n",
    "    y_pred = model.predict_classes(x_test[test])\n",
    "\n",
    "    sum_all_j = 0\n",
    "    sum_all_s = 0\n",
    "    sum_j = 0\n",
    "    sum_s = 0\n",
    "\n",
    "    for i in range(x_test[test].shape[0]):\n",
    "        #print(\"%s:%s\" % (y_pred[i], Y_eva[test][i]))\n",
    "        if y_pred[i] == 1:\n",
    "            if y_pred[i] == Y_eva[test][i]:           \n",
    "                sum_j = sum_j + 1  \n",
    "\n",
    "            sum_all_j = sum_all_j + 1  \n",
    "\n",
    "        else:\n",
    "            if y_pred[i] == Y_eva[test][i]:\n",
    "                sum_s = sum_s + 1\n",
    "\n",
    "            sum_all_s = sum_all_s + 1\n",
    "\n",
    "\n",
    "    print(\"人工物画像の正解枚数：\",sum_j,\"/\",sum_all_j)\n",
    "    print(\"自然物画像の正解枚数：\",sum_s,\"/\",sum_all_s)\n",
    "    \n",
    "    sum_j =  sum_j / sum_all_j * 100\n",
    "    sum_s =  sum_s / sum_all_s * 100\n",
    "    ave = (sum_s + sum_j) / 2 + ave\n",
    "\n",
    "    print(\"人工物画像の正解率＝\",sum_j,\"%\")\n",
    "    print(\"自然物画像の正解率＝\",sum_s,\"%\")\n",
    "    print(\"全体画像の正解率＝\",(sum_s + sum_j) / 2,\"%\")\n",
    "      \n",
    "    # Evaluate\n",
    "    scores = model.evaluate(x_test[test], keras.utils.to_categorical(Y_eva[test], num_classes), verbose=0)\n",
    "    print(x_test[test].shape)\n",
    "    #print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    #cvscores.append(scores[1] * 100)\n",
    "    cvscores.append((sum_s + sum_j) / 2)\n",
    "\n",
    "print(\"平均正解率：%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.training.Model object at 0x7ffa930e2748>\n",
      "<keras.engine.topology.InputLayer object at 0x7ffa9324c4a8>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 40,407,874\n",
      "Trainable params: 32,772,610\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "Train on 800 samples, validate on 1600 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.6749 - acc: 0.5875 - val_loss: 0.5206 - val_acc: 0.8447\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.4386 - acc: 0.8681 - val_loss: 0.3390 - val_acc: 0.9428\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3000 - acc: 0.9237 - val_loss: 0.2326 - val_acc: 0.9487\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.2084 - acc: 0.9419 - val_loss: 0.1794 - val_acc: 0.9528\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1742 - acc: 0.9462 - val_loss: 0.1531 - val_acc: 0.9531\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1496 - acc: 0.9525 - val_loss: 0.1416 - val_acc: 0.9563\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1354 - acc: 0.9556 - val_loss: 0.1301 - val_acc: 0.9553\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.1294 - acc: 0.9563 - val_loss: 0.1251 - val_acc: 0.9603\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1145 - acc: 0.9637 - val_loss: 0.1204 - val_acc: 0.9606\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1127 - acc: 0.9650 - val_loss: 0.1183 - val_acc: 0.9616\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1029 - acc: 0.9694 - val_loss: 0.1159 - val_acc: 0.9622\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1049 - acc: 0.9675 - val_loss: 0.1138 - val_acc: 0.9622\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0978 - acc: 0.9688 - val_loss: 0.1170 - val_acc: 0.9609\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0951 - acc: 0.9719 - val_loss: 0.1123 - val_acc: 0.9625\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0899 - acc: 0.9731 - val_loss: 0.1098 - val_acc: 0.9634\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0831 - acc: 0.9756 - val_loss: 0.1088 - val_acc: 0.9641\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0854 - acc: 0.9706 - val_loss: 0.1165 - val_acc: 0.9603\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0832 - acc: 0.9713 - val_loss: 0.1068 - val_acc: 0.9631\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0719 - acc: 0.9806 - val_loss: 0.1063 - val_acc: 0.9644\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0749 - acc: 0.9769 - val_loss: 0.1054 - val_acc: 0.9637\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0711 - acc: 0.9819 - val_loss: 0.1049 - val_acc: 0.9631\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0638 - acc: 0.9806 - val_loss: 0.1048 - val_acc: 0.9631\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0624 - acc: 0.9844 - val_loss: 0.1081 - val_acc: 0.9659\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0659 - acc: 0.9831 - val_loss: 0.1036 - val_acc: 0.9637\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0580 - acc: 0.9838 - val_loss: 0.1034 - val_acc: 0.9637\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0580 - acc: 0.9869 - val_loss: 0.1030 - val_acc: 0.9625\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0568 - acc: 0.9831 - val_loss: 0.1028 - val_acc: 0.9628\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0547 - acc: 0.9844 - val_loss: 0.1021 - val_acc: 0.9637\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0505 - acc: 0.9844 - val_loss: 0.1031 - val_acc: 0.9631\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0475 - acc: 0.9875 - val_loss: 0.1017 - val_acc: 0.9634\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0492 - acc: 0.9856 - val_loss: 0.1017 - val_acc: 0.9650\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0491 - acc: 0.9856 - val_loss: 0.1018 - val_acc: 0.9631\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0417 - acc: 0.9888 - val_loss: 0.1044 - val_acc: 0.9644\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0461 - acc: 0.9894 - val_loss: 0.1024 - val_acc: 0.9631\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.1016 - val_acc: 0.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0396 - acc: 0.9913 - val_loss: 0.1018 - val_acc: 0.9634\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0397 - acc: 0.9900 - val_loss: 0.1021 - val_acc: 0.9634\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0391 - acc: 0.9894 - val_loss: 0.1014 - val_acc: 0.9644\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0368 - acc: 0.9894 - val_loss: 0.1018 - val_acc: 0.9637\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0346 - acc: 0.9919 - val_loss: 0.1022 - val_acc: 0.9650\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0371 - acc: 0.9919 - val_loss: 0.1016 - val_acc: 0.9641\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0345 - acc: 0.9925 - val_loss: 0.1017 - val_acc: 0.9637\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0306 - acc: 0.9931 - val_loss: 0.1024 - val_acc: 0.9663\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0316 - acc: 0.9931 - val_loss: 0.1022 - val_acc: 0.9641\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0302 - acc: 0.9913 - val_loss: 0.1022 - val_acc: 0.9656\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0297 - acc: 0.9919 - val_loss: 0.1031 - val_acc: 0.9650\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0280 - acc: 0.9937 - val_loss: 0.1027 - val_acc: 0.9647\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0266 - acc: 0.9944 - val_loss: 0.1023 - val_acc: 0.9641\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0264 - acc: 0.9962 - val_loss: 0.1051 - val_acc: 0.9641\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0269 - acc: 0.9950 - val_loss: 0.1027 - val_acc: 0.9647\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0244 - acc: 0.9950 - val_loss: 0.1030 - val_acc: 0.9656\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0266 - acc: 0.9937 - val_loss: 0.1032 - val_acc: 0.9644\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0212 - acc: 0.9956 - val_loss: 0.1066 - val_acc: 0.9663\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0251 - acc: 0.9950 - val_loss: 0.1070 - val_acc: 0.9641\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0212 - acc: 0.9969 - val_loss: 0.1041 - val_acc: 0.9663\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0211 - acc: 0.9950 - val_loss: 0.1030 - val_acc: 0.9650\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0211 - acc: 0.9981 - val_loss: 0.1034 - val_acc: 0.9637\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0190 - acc: 0.9969 - val_loss: 0.1043 - val_acc: 0.9637\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0179 - acc: 0.9988 - val_loss: 0.1040 - val_acc: 0.9659\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0191 - acc: 0.9975 - val_loss: 0.1041 - val_acc: 0.9650\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0179 - acc: 0.9981 - val_loss: 0.1046 - val_acc: 0.9637\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0171 - acc: 0.9981 - val_loss: 0.1048 - val_acc: 0.9644\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0169 - acc: 0.9969 - val_loss: 0.1050 - val_acc: 0.9650\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0171 - acc: 0.9981 - val_loss: 0.1077 - val_acc: 0.9663\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0146 - acc: 0.9994 - val_loss: 0.1059 - val_acc: 0.9644\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0146 - acc: 0.9994 - val_loss: 0.1058 - val_acc: 0.9647\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0140 - acc: 0.9975 - val_loss: 0.1061 - val_acc: 0.9650\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0136 - acc: 0.9988 - val_loss: 0.1066 - val_acc: 0.9647\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0144 - acc: 0.9988 - val_loss: 0.1072 - val_acc: 0.9641\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0134 - acc: 0.9988 - val_loss: 0.1075 - val_acc: 0.9656\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0138 - acc: 0.9988 - val_loss: 0.1086 - val_acc: 0.9641\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0133 - acc: 0.9988 - val_loss: 0.1078 - val_acc: 0.9641\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0123 - acc: 0.9994 - val_loss: 0.1086 - val_acc: 0.9653\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0126 - acc: 0.9981 - val_loss: 0.1089 - val_acc: 0.9647\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0122 - acc: 0.9988 - val_loss: 0.1084 - val_acc: 0.9656\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.1105 - val_acc: 0.9634\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0119 - acc: 0.9988 - val_loss: 0.1091 - val_acc: 0.9647\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.1098 - val_acc: 0.9644\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0103 - acc: 0.9994 - val_loss: 0.1100 - val_acc: 0.9644\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0113 - acc: 0.9988 - val_loss: 0.1098 - val_acc: 0.9650\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0103 - acc: 0.9994 - val_loss: 0.1105 - val_acc: 0.9644\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0096 - acc: 0.9994 - val_loss: 0.1107 - val_acc: 0.9644\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0105 - acc: 0.9988 - val_loss: 0.1105 - val_acc: 0.9659\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.1120 - val_acc: 0.9644\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.1110 - val_acc: 0.9659\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0092 - acc: 0.9988 - val_loss: 0.1113 - val_acc: 0.9656\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.1118 - val_acc: 0.9647\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.1118 - val_acc: 0.9666\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0096 - acc: 0.9988 - val_loss: 0.1123 - val_acc: 0.9650\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0081 - acc: 0.9994 - val_loss: 0.1140 - val_acc: 0.9644\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.1130 - val_acc: 0.9641\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.1128 - val_acc: 0.9666\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0086 - acc: 0.9994 - val_loss: 0.1133 - val_acc: 0.9647\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 0.9663\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.1152 - val_acc: 0.9647\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9650\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.1141 - val_acc: 0.9663\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0075 - acc: 0.9994 - val_loss: 0.1143 - val_acc: 0.9659\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.1145 - val_acc: 0.9663\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.1154 - val_acc: 0.9641\n",
      "Saved trained model at /home/tsunei/master/CNN/saved_models/keras_vgg16_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "### モデル構築 \n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "#vgg_conv.summary()\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "print(vgg_conv)\n",
    "print(vgg_conv.layers[0])\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv.layers[0])\n",
    "model.add(vgg_conv.layers[1])\n",
    "model.add(vgg_conv.layers[2])\n",
    "model.add(vgg_conv.layers[3])\n",
    "model.add(vgg_conv.layers[4])\n",
    "model.add(vgg_conv.layers[5])\n",
    "model.add(vgg_conv.layers[6])\n",
    "model.add(vgg_conv.layers[7])\n",
    "model.add(vgg_conv.layers[8])\n",
    "model.add(vgg_conv.layers[9])\n",
    "model.add(vgg_conv.layers[10])\n",
    "model.add(vgg_conv.layers[11])\n",
    "model.add(vgg_conv.layers[12])\n",
    "model.add(vgg_conv.layers[13])\n",
    "model.add(vgg_conv.layers[14])\n",
    "model.add(vgg_conv.layers[15])\n",
    "model.add(vgg_conv.layers[16])\n",
    "model.add(vgg_conv.layers[17])\n",
    "model.add(vgg_conv.layers[18])\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))   ### データは４種類\n",
    " \n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "### 学習\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=True)\n",
    " \n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 4s 3ms/step\n",
      "Test loss: 0.115434223896591\n",
      "Test accuracy: 0.9640625\n",
      "人工物画像 95.375 %\n",
      "自然物画像 97.5 %\n",
      "全体画像 96.4375 %\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    " \n",
    " \n",
    "### Plot accuracy &amp; loss\n",
    "import matplotlib.pyplot as plt \n",
    " \n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    " \n",
    "epochs = range(1, len(acc) + 1)\n",
    " \n",
    "#plot accuracy\n",
    "plt.plot(epochs, acc, \"bo\", label = \"Training acc\" )\n",
    "plt.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\n",
    "plt.title(\"Training and Validation accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"acc.png\")\n",
    "plt.close()  \n",
    "\n",
    "#plot loss\n",
    "plt.plot(epochs, loss, \"bo\", label = \"Training loss\" )\n",
    "plt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss.png\")\n",
    "plt.close()\n",
    "\n",
    "# 正誤表\n",
    "y_pred = model.predict_classes(x_test)\n",
    "\n",
    "sum_j = 0\n",
    "sum_s = 0\n",
    "for i in range(x_test.shape[0]):\n",
    "    if i < (x_test.shape[0] / 2):\n",
    "        if y_pred[i] == 0:\n",
    "            sum_j = sum_j + 1\n",
    "            \n",
    "    else:\n",
    "        if y_pred[i] == 1:\n",
    "            sum_s = sum_s + 1\n",
    "                     \n",
    "print(\"人工物画像\",sum_j / (x_test.shape[0] / 2) * 100,\"%\")\n",
    "print(\"自然物画像\",sum_s / (x_test.shape[0] / 2) * 100,\"%\")\n",
    "print(\"全体画像\",(sum_s + sum_j) / x_test.shape[0] * 100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
